{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Diligence Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate multiple models of students with varying noise and compare diligence estimates with each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import uuid\n",
    "import os\n",
    "import copy\n",
    "import itertools\n",
    "from collections.abc import Iterable\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import Bounds\n",
    "from scipy.optimize import minimize\n",
    "from scipy import optimize\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "#logging.basicConfig(level=logging.DEBUG)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "#logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "logger = logging.getLogger(\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logging.getLogger().setLevel(logging.DEBUG)\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "#logging.getLogger().setLevel(logging.WARNING)\n",
    "logger.debug(\"Test debug\")\n",
    "logger.info(\"Test info\")\n",
    "logger.warning(\"Test warning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tutor.domain import Domain\n",
    "from tutor.curriculum_factory import CurriculumFactory\n",
    "from tutor.simple_curriculum import SimpleCurriculum\n",
    "from tutor.tutor import SimpleTutor\n",
    "from tutor.action import Attempt, HintRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learner.selfeff_learner import SelfEfficacyLearner\n",
    "from learner.modular_learner import ModularLearner\n",
    "from learner.cognition import *\n",
    "from learner.decider import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulate.modlearner_simulation import ModLearnerSimulation\n",
    "from simulate.simulation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analytics.student_stats import StudentStatCalc\n",
    "from analytics.cae import *\n",
    "from analytics.featurization import *\n",
    "from analytics.batch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from log_db import mongo\n",
    "from log_db.curriculum_mapper import DB_Curriculum_Mapper\n",
    "from log_db.learner_mapper import DBLearnerMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CanonicalAutocorrelationAnalysis.model.caa import CAAComputation\n",
    "from CanonicalAutocorrelationAnalysis.model.caaObject import *\n",
    "from CanonicalAutocorrelationAnalysis.model.utils import l1Norm, l2Norm, r2Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path to current project directory\n",
    "cwd = os.path.abspath(\".\")\n",
    "base_dir = os.path.abspath(os.path.join(cwd, os.pardir))\n",
    "logger.info(\"Base directory for the project:\\n%s\" % base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup connection to database\n",
    "data_out = \"sim-%s\" % str(uuid.uuid4())\n",
    "data_path = os.path.join(base_dir,\"test\", \"data\", data_out)\n",
    "logger.info(\"Writing simulation results to directory: %s\" % data_path)\n",
    "db_name = \"motivsim\"\n",
    "db_params  = mongo.get_db_params(db_name)\n",
    "logger.info(\"got db params: %s\" % str(db_params))\n",
    "db_util = mongo.Data_Utility(data_path, db_params)\n",
    "db = db_util.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test db connection\n",
    "db_util.peak()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_db = True\n",
    "if clear_db:\n",
    "    logger.info(\"Clearing database before starting new simulation\")\n",
    "    db_util.clear_db()\n",
    "else:\n",
    "    logger.info(\"Skipping Clearing database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulating learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_students = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cog_params():\n",
    "    ability = -2\n",
    "    while (ability < -1) or (ability > 1):\n",
    "        ability = np.random.normal(0, 0.6)\n",
    "    return {\"ability\": ability}\n",
    "\n",
    "def gen_students(num_students, domain, curric, \n",
    "                 cog_mod, cog_params, dec_mod, dec_params):\n",
    "    stus = []\n",
    "    for i in range(num_students):\n",
    "        cp = cog_params()\n",
    "        cog = cog_mod(domain, **cp)\n",
    "        dp = dec_params()\n",
    "        dec = dec_mod(**dp)\n",
    "        decider = DiligentDecider(dec)\n",
    "        stu = ModularLearner(domain, cog, decider)\n",
    "        stus.append(stu)\n",
    "        \n",
    "    return stus\n",
    "\n",
    "def simulate_students(curric, students, batch):    \n",
    "    \n",
    "    env = simpy.Environment()\n",
    "\n",
    "    mastery_thres = 0.9\n",
    "    m_ses_len = 40\n",
    "    sd_ses_len = 8\n",
    "    max_ses_len = 60\n",
    "    sim_start = dt.datetime.now()\n",
    "\n",
    "    \n",
    "    for i, stu in enumerate(students):\n",
    "        logger.info(\"Simulating student #%i\" % i)\n",
    "        # Create associated tutor\n",
    "        tutor = SimpleTutor(curric, stu._id, mastery_thres)\n",
    "\n",
    "        # Initialize simulation processes\n",
    "        num_sessions = 3\n",
    "        sim = SingleStudentSim(env, sim_start, stu, tutor,\n",
    "                               num_sessions, m_ses_len, sd_ses_len, max_ses_len)\n",
    "        batch.add_sim(sim)\n",
    "\n",
    "        env.process(sim.run())\n",
    "\n",
    "    env.run()\n",
    "                \n",
    "    logger.info(\"Inserting %i simulated students to db\" % len(students))\n",
    "    result = db.finalsimstudents.insert_many([stu.to_dict() for stu in students])\n",
    "    logger.info(\"Db insert success: %s\" % result.acknowledged)\n",
    "\n",
    "    logger.info(\"Inserting simulation batch to db\")\n",
    "    result = db.simbatches.insert_one(batch.to_dict())\n",
    "    logger.info(\"Db insert success: %s\" % result.acknowledged)\n",
    "\n",
    "    return batch, students\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Curriculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_curric = None\n",
    "new_domain = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test_curric(db, db_params):\n",
    "    global new_curric, new_domain\n",
    "    if new_curric is None:\n",
    "        logger.info(\"Generating new curriculum\")\n",
    "        domain_params = {'m_l0': 0.45,\n",
    "                         'sd_l0': 0.155,\n",
    "                         'm_l0_sd': 0.1,\n",
    "                         'sd_l0_sd': 0.03,\n",
    "                         'm_t': 0.35,\n",
    "                         'sd_t': 0.13,#0.03,\n",
    "                         'm_s': 0.105,\n",
    "                         'sd_s': 0.055,\n",
    "                         'm_g': 0.45,#0.6,\n",
    "                         'sd_g': 0.105\n",
    "                            }\n",
    "        curric_params = {'num_units': 2,\n",
    "                         'mean_sections': 4,\n",
    "                         'stdev_sections': 2,\n",
    "                         'mean_unit_kcs': 22,\n",
    "                         'stdev_unit_kcs': 23,\n",
    "                         'section_kcs_lambda': 6,\n",
    "                         'mean_steps': 10,\n",
    "                         'stdev_steps': 4,\n",
    "                         'mean_prob_kcs': 6,\n",
    "                         'stdev_prob_kcs': 3,\n",
    "                         'num_practice': 100\n",
    "                        }\n",
    "\n",
    "        domain, curric = CurriculumFactory.gen_curriculum(domain_params, curric_params)\n",
    "        db.domains.insert_one(domain.to_dict())\n",
    "        db.kcs.insert_many([kc.__dict__ for kc in domain.kcs])\n",
    "        curric_util = DB_Curriculum_Mapper(db_params)\n",
    "        curric_util.write_to_db(curric)\n",
    "        \n",
    "        new_curric = curric\n",
    "        new_domain = domain\n",
    "\n",
    "        return domain, curric\n",
    "    else:\n",
    "        logger.info(\"New curriculum already generated\")\n",
    "        return new_domain, new_curric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Diligent students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_batch_desc = \"Simple diligent students\"\n",
    "cog_mod = BiasSkillCognition\n",
    "dec_mod = EVDecider\n",
    "\n",
    "def get_cog_params():\n",
    "    # Helper for getting parameters for BiasSkillCognition Module\n",
    "    return {'ability': random.triangular(-1,1)}\n",
    "\n",
    "def get_dec_params():\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simbatch = db.simbatches.find_one({\"desc\": sim_batch_desc})\n",
    "if simbatch is None:\n",
    "    logger.info(\"Generating new simulation. None found in db\")\n",
    "\n",
    "    # generate simualted data for test\n",
    "    domain, curric = gen_test_curric(db, db_params)\n",
    "    students = gen_students(num_students, domain, curric, \n",
    "                            cog_mod, get_cog_params, \n",
    "                            dec_mod, get_dec_params)   \n",
    "    logger.info(f\"Persisting {len(students)} initialized students to db\")\n",
    "    db.students.insert_many([stu.to_dict() for stu in students])\n",
    "    batch = SimulationBatch(sim_batch_desc)\n",
    "    simulate_students(curric, students, batch)    \n",
    "    logger.info(f\"Simulated {len(students)} in batch with id: {batch._id}\")\n",
    "else:\n",
    "    logger.info(f\"Found simulation batch: {str(simbatch)}\")\n",
    "    lmapper = DBLearnerMapper(db)\n",
    "    students = [lmapper.get_modlearner_from_db(sid) for sid in simbatch['student_ids']]\n",
    "    batch = SimulationBatch.from_dict(simbatch)\n",
    "    logger.info(f\"Recovered {len(students)} students from batch with id: {batch._id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diligent Students with variable values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_batch_desc = \"Diligent Students with variable values\"\n",
    "cog_mod = BinarySkillCognition\n",
    "dec_mod = RandValDecider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simbatch = db.simbatches.find_one({\"desc\": sim_batch_desc})\n",
    "if simbatch is None:\n",
    "    logger.info(\"Generating new simulation. None found in db\")\n",
    "\n",
    "    # generate simualted data for test\n",
    "    domain, curric = gen_test_curric(db, db_params)\n",
    "    students = gen_students(num_students, \n",
    "                            domain, curric,\n",
    "                            cog_mod, dec_mod)    \n",
    "    logger.info(f\"Persisting {len(students)} initialized students to db\")\n",
    "    db.students.insert_many([stu.to_dict() for stu in students])\n",
    "    batch = SimulationBatch(sim_batch_desc)\n",
    "    simulate_students(domain, curric, students, batch)    \n",
    "    logger.info(f\"Simulated {len(students)} in batch with id: {batch._id}\")\n",
    "else:\n",
    "    logger.info(f\"Found simulation batch: {str(simbatch)}\")\n",
    "    lmapper = DBLearnerMapper(db)\n",
    "    students = [lmapper.get_modlearner_from_db(sid) for sid in simbatch['student_ids']]\n",
    "    batch = SimulationBatch.from_dict(simbatch)\n",
    "    logger.info(f\"Recovered {len(students)} students from batch with id: {batch._id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diligent Students with domain-level self-efficacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_batch_desc = \"Diligent Students with domain-level self-efficacy\"\n",
    "cog_mod = BinarySkillCognition\n",
    "dec_mod = DomainSelfEffDecider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simbatch = db.simbatches.find_one({\"desc\": sim_batch_desc})\n",
    "if simbatch is None:\n",
    "    logger.info(\"Generating new simulation. None found in db\")\n",
    "\n",
    "    # generate simualted data for test\n",
    "    domain, curric = gen_test_curric(db, db_params)\n",
    "    students = gen_students(num_students, \n",
    "                            domain, curric,\n",
    "                            cog_mod, dec_mod)    \n",
    "    logger.info(f\"Persisting {len(students)} initialized students to db\")\n",
    "    db.students.insert_many([stu.to_dict() for stu in students])\n",
    "    batch = SimulationBatch(sim_batch_desc)\n",
    "    simulate_students(domain, curric, students, batch)    \n",
    "    logger.info(f\"Simulated {len(students)} in batch with id: {batch._id}\")\n",
    "else:\n",
    "    logger.info(f\"Found simulation batch: {str(simbatch)}\")\n",
    "    lmapper = DBLearnerMapper(db)\n",
    "    students = [lmapper.get_modlearner_from_db(sid) for sid in simbatch['student_ids']]\n",
    "    batch = SimulationBatch.from_dict(simbatch)\n",
    "    logger.info(f\"Recovered {len(students)} students from batch with id: {batch._id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA of simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List size of all collections\n",
    "db_util.peak()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get students batches\n",
    "batches = [batch for batch in db.simbatches.find()]\n",
    "batch_desc = [\"Simple diligent students\",\n",
    "              \"Diligent Students with variable values\",\n",
    "              \"Diligent Students with domain-level self-efficacy\"]\n",
    "sids = {desc: [] for desc in batch_desc}\n",
    "for i, batch in enumerate(batches):\n",
    "    logger.info(f\"batch #{i}: \\nID: {batch['_id']}\\ndesc: {batch['desc']}\")\n",
    "    if batch['desc'] in batch_desc:\n",
    "        logger.info(f\"recovered {len(batch['student_ids'])} student ids for batch {batch['desc']}\")\n",
    "        sids[batch['desc']].append(batch['student_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch analytic calculation\n",
    "def batch_calc(db, calc, filt, batch_size, ):\n",
    "    #filt is a list of ids\n",
    "    batches = (filt[i:i+batch_size] for i in range(0, len(filt), batch_size))\n",
    "    results = []\n",
    "    for batch in batches:\n",
    "        results.append(calc(batch))\n",
    "    return pd.concat(results, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_calc(calc, args):\n",
    "    start = dt.datetime.now()\n",
    "    result = calc(*args)\n",
    "    end = dt.datetime.now()\n",
    "    runtime = (end - start).total_seconds()\n",
    "    return result, runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tx_fields(sids):\n",
    "    tx = pd.DataFrame(db.tutor_events.find({\"stu_id\": {'$in': sids}}))\n",
    "    # Add kc field that reduces list of kcs to 1 kc\n",
    "    tx['kc'] = tx.apply(lambda x: x['kcs'][0]['_id'], axis=1)\n",
    "    return tx.loc[:, [\"_id\", 'stu_id', 'kc', 'unit_id', 'section_id', 'prob_id', 'step_id', \"duration\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating stats with analytic methods\n",
    "\n",
    "calc = StudentStatCalc(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Diligent Students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = sids[batch_desc[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [db, calc.get_stu_parameters, sid, 10]\n",
    "sim_students, runtime = time_calc(batch_calc, args)\n",
    "logger.info(f\"Calculated student params: {sim_students.shape}\\tRuntime: {runtime} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [db, calc.action_stats, sid, 10]\n",
    "action_dist, runtime = time_calc(batch_calc, args)\n",
    "logger.info(f\"Calculated student action stats: {action_dist.shape}\\tRuntime: {runtime} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [db, calc.total_tx_stats, sid, 10]\n",
    "tx_stats, runtime = time_calc(batch_calc, args)\n",
    "logger.info(f\"Calculated student activity stats: {tx_stats.shape}\\tRuntime: {runtime} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "stu_stats = pd.concat([sim_students, action_dist, tx_stats], axis=1)\n",
    "logger.info(f\"Merged new stats together: {stu_stats.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sim_students.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mastery\n",
    "plt.figure(figsize=(10,2))\n",
    "plt.subplot(131)\n",
    "plt.hist(sim_students['pre-sim pct mastery'], bins=10)\n",
    "plt.title(\"pre-sim pct mastery\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hist(sim_students['final-sim pct mastery'], bins=10)\n",
    "plt.title(\"final-sim pct mastery\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.hist(sim_students['final-sim total unmastered'], bins=10)\n",
    "plt.title(\"final-sim total unmastered\")\n",
    "\n",
    "plt.show()\n",
    "logger.info(\"Total skills: %i\" % sim_students['total skills'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Values & Diligence\n",
    "plt.figure(figsize=(8,7))\n",
    "for i,val in enumerate(sim_students['values'][0].keys()):\n",
    "    print(\"%i: %s\" % (i, val))\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.hist(sim_students['values'].apply(lambda x: x[val]), bins=10)\n",
    "    plt.title('Student value for %s' % val)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.hist(sim_students['diligence'], bins=10)\n",
    "plt.title(\"Diligence\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learner Decisions EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Learner decisions\n",
    "if \"stus_1\" in globals():\n",
    "    decisions = pd.DataFrame(db.decisions.find({\"student_id\": {'$in': [stu._id for stu in stus_1]}}))\n",
    "else:\n",
    "    decisions = pd.DataFrame(db.decisions.find({\"student_id\": {'$in': sid}}))\n",
    "decisions['learner_knowledge'] = decisions['learner_knowledge'].apply(lambda x: x[0] if isinstance(x, Iterable) else x)\n",
    "decisions['kcid'] = decisions['kc'].apply(lambda x: x['_id'])\n",
    "decisions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learner Actions EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,2))\n",
    "plt.subplot(141)\n",
    "plt.hist(action_dist['Pct Attempt'], bins=10)\n",
    "plt.title(\"Pct Attempt\")\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.hist(action_dist['Pct Guess'], bins=10)\n",
    "plt.title(\"Pct Guess\")\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.hist(action_dist['Pct Hint Request'], bins=10)\n",
    "plt.title(\"Pct Hint Request\")\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.hist(action_dist['Pct Off Task'], bins=10)\n",
    "plt.title(\"Pct Off Task\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learner Transactions EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [db, get_tx_fields, sid, 10]\n",
    "tx, runtime = time_calc(batch_calc, args)\n",
    "logger.info(f\"Extracted tx for set of students in {runtime} seconds: {tx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_stats = tx.groupby(['stu_id', 'unit_id', 'section_id', 'prob_id', 'step_id'])['duration'].agg(['sum', 'count']).reset_index()\n",
    "stu_prob_stats = step_stats.groupby('stu_id')['count'].describe()\n",
    "stu_prob_stats.columns = [\"Step Attempt %s\" % col for col in stu_prob_stats.columns]\n",
    "d = step_stats.groupby('stu_id')['sum'].describe()\n",
    "d.columns = [\"Step Duration %s\" % col for col in d.columns]\n",
    "stu_prob_stats = pd.concat([stu_prob_stats, d], axis=1)\n",
    "stu_prob_stats.head()\n",
    "\n",
    "# kc_stats = tx[['stu_id', 'kc', 'step_id']].drop_duplicates().groupby(['stu_id', 'kc']).count()\n",
    "stu_kc_stats = tx[['stu_id', 'kc', 'step_id']].drop_duplicates().groupby(['stu_id', 'kc']).count().reset_index()\n",
    "stu_kc_stats.rename(columns={'step_id': 'kc opportunities'}, inplace=True)\n",
    "kc_stats = stu_kc_stats.groupby('kc').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,2))\n",
    "plt.subplot(121)\n",
    "plt.hist(tx_stats['Total Tx'], bins=10)\n",
    "plt.title(\"Total Tx\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(tx_stats['Total Time(hours)'], bins=10)\n",
    "plt.title(\"Total Time(hours)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,2))\n",
    "plt.subplot(131)\n",
    "plt.hist(tx_stats['Pct Correct'], bins=10)\n",
    "plt.title(\"Pct Correct Tx\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hist(tx_stats['Pct Incorrect'], bins=10)\n",
    "plt.title(\"Pct Incorrect Tx\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.hist(tx_stats['Pct Hint'], bins=10)\n",
    "plt.title(\"Pct Hint Tx\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.hist(tx['duration'],bins=50)\n",
    "plt.title(\"Tx duration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 10\n",
    "num_rows = 3\n",
    "num_cols = 6\n",
    "row_height = 3\n",
    "col_width = 3\n",
    "plt.figure(figsize=(col_width*num_cols, row_height*num_rows+num_rows))\n",
    "\n",
    "row = 0\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 1)\n",
    "plt.hist(stu_prob_stats['Step Attempt mean'], bins=num_bins)\n",
    "plt.title(\"Mean Attempts per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 2)\n",
    "plt.hist(stu_prob_stats['Step Attempt std'], bins=num_bins)\n",
    "plt.title(\"Standard Dev Attempts per step\")\n",
    "\n",
    "\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 3)\n",
    "plt.hist(stu_prob_stats['Step Attempt 25%'], bins=num_bins)\n",
    "plt.title(\"Q1 Attempts per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 4)\n",
    "plt.hist(stu_prob_stats['Step Attempt 50%'], bins=num_bins)\n",
    "plt.title(\"Q2 Attempts per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 5)\n",
    "plt.hist(stu_prob_stats['Step Attempt 75%'], bins=num_bins)\n",
    "plt.title(\"Q3 Attempts per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 6)\n",
    "plt.hist(stu_prob_stats['Step Attempt max'], bins=num_bins)\n",
    "plt.title(\"Max Attempts per step\")\n",
    "\n",
    "row = 1\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 1)\n",
    "plt.hist(stu_prob_stats['Step Duration mean'], bins=num_bins)\n",
    "plt.title(\"Mean Time per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 2)\n",
    "plt.hist(stu_prob_stats['Step Duration std'], bins=num_bins)\n",
    "plt.title(\"Standard Dev Time per step\")\n",
    "\n",
    "\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 3)\n",
    "plt.hist(stu_prob_stats['Step Duration 25%'], bins=num_bins)\n",
    "plt.title(\"Q1 Time per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 4)\n",
    "plt.hist(stu_prob_stats['Step Duration 50%'], bins=num_bins)\n",
    "plt.title(\"Q2 Time per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 5)\n",
    "plt.hist(stu_prob_stats['Step Duration 75%'], bins=num_bins)\n",
    "plt.title(\"Q3 Time per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 6)\n",
    "plt.hist(stu_prob_stats['Step Duration max'], bins=num_bins)\n",
    "plt.title(\"Max Time per step\")\n",
    "\n",
    "row = 2\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 1)\n",
    "plt.hist(kc_stats[('kc opportunities', 'mean')], bins=num_bins)\n",
    "plt.title(\"Mean opportunities per kc\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 2)\n",
    "plt.hist(kc_stats[('kc opportunities', 'std')], bins=num_bins)\n",
    "plt.title(\"Standard Dev opportunities per kc\")\n",
    "\n",
    "\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 3)\n",
    "plt.hist(kc_stats[('kc opportunities', '25%')], bins=num_bins)\n",
    "plt.title(\"Q1 opportunities per kc\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 4)\n",
    "plt.hist(kc_stats[('kc opportunities', '50%')], bins=num_bins)\n",
    "plt.title(\"Q2 opportunities per kc\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 5)\n",
    "plt.hist(kc_stats[('kc opportunities', '75%')], bins=num_bins)\n",
    "plt.title(\"Q3 opportunities per kc\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 6)\n",
    "plt.hist(kc_stats[('kc opportunities', 'max')], bins=num_bins)\n",
    "plt.title(\"Max opportunities per kc\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diligent with variable values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = sids[batch_desc[1]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [db, calc.get_stu_parameters, sid, 10]\n",
    "sim_students, runtime = time_calc(batch_calc, args)\n",
    "logger.info(f\"Calculated student params: {sim_students.shape}\\tRuntime: {runtime} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [db, calc.action_stats, sid, 10]\n",
    "action_dist, runtime = time_calc(batch_calc, args)\n",
    "logger.info(f\"Calculated student action stats: {action_dist.shape}\\tRuntime: {runtime} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [db, calc.total_tx_stats, sid, 10]\n",
    "tx_stats, runtime = time_calc(batch_calc, args)\n",
    "logger.info(f\"Calculated student activity stats: {tx_stats.shape}\\tRuntime: {runtime} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "stu_stats = pd.concat([sim_students, action_dist, tx_stats], axis=1)\n",
    "logger.info(f\"Merged new stats together: {stu_stats.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_students.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mastery\n",
    "plt.figure(figsize=(10,2))\n",
    "plt.subplot(131)\n",
    "plt.hist(sim_students['pre-sim pct mastery'], bins=10)\n",
    "plt.title(\"pre-sim pct mastery\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hist(sim_students['final-sim pct mastery'], bins=10)\n",
    "plt.title(\"final-sim pct mastery\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.hist(sim_students['final-sim total unmastered'], bins=10)\n",
    "plt.title(\"final-sim total unmastered\")\n",
    "\n",
    "plt.show()\n",
    "logger.info(\"Total skills: %i\" % sim_students['total skills'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Values & Diligence\n",
    "plt.figure(figsize=(8,7))\n",
    "for i,val in enumerate(sim_students['values'][0].keys()):\n",
    "    print(\"%i: %s\" % (i, val))\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.hist(sim_students['values'].apply(lambda x: x[val]), bins=10)\n",
    "    plt.title('Student value for %s' % val)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.hist(sim_students['diligence'], bins=10)\n",
    "plt.title(\"Diligence\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learner Actions EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,2))\n",
    "plt.subplot(141)\n",
    "plt.hist(action_dist['Pct Attempt'], bins=10)\n",
    "plt.title(\"Pct Attempt\")\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.hist(action_dist['Pct Guess'], bins=10)\n",
    "plt.title(\"Pct Guess\")\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.hist(action_dist['Pct Hint Request'], bins=10)\n",
    "plt.title(\"Pct Hint Request\")\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.hist(action_dist['Pct Off Task'], bins=10)\n",
    "plt.title(\"Pct Off Task\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learner Transactions EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [db, get_tx_fields, sid, 10]\n",
    "tx, runtime = time_calc(batch_calc, args)\n",
    "logger.info(f\"Extracted tx for set of students in {runtime} seconds: {tx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_stats = tx.groupby(['stu_id', 'unit_id', 'section_id', 'prob_id', 'step_id'])['duration'].agg(['sum', 'count']).reset_index()\n",
    "stu_prob_stats = step_stats.groupby('stu_id')['count'].describe()\n",
    "stu_prob_stats.columns = [\"Step Attempt %s\" % col for col in stu_prob_stats.columns]\n",
    "d = step_stats.groupby('stu_id')['sum'].describe()\n",
    "d.columns = [\"Step Duration %s\" % col for col in d.columns]\n",
    "stu_prob_stats = pd.concat([stu_prob_stats, d], axis=1)\n",
    "stu_prob_stats.head()\n",
    "\n",
    "# kc_stats = tx[['stu_id', 'kc', 'step_id']].drop_duplicates().groupby(['stu_id', 'kc']).count()\n",
    "stu_kc_stats = tx[['stu_id', 'kc', 'step_id']].drop_duplicates().groupby(['stu_id', 'kc']).count().reset_index()\n",
    "stu_kc_stats.rename(columns={'step_id': 'kc opportunities'}, inplace=True)\n",
    "kc_stats = stu_kc_stats.groupby('kc').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,2))\n",
    "plt.subplot(121)\n",
    "plt.hist(stu_stats['Total Tx'], bins=10)\n",
    "plt.title(\"Total Tx\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(stu_stats['Total Time(hours)'], bins=10)\n",
    "plt.title(\"Total Time(hours)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,2))\n",
    "plt.subplot(131)\n",
    "plt.hist(stu_stats['Pct Correct'], bins=10)\n",
    "plt.title(\"Pct Correct Tx\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hist(stu_stats['Pct Incorrect'], bins=10)\n",
    "plt.title(\"Pct Incorrect Tx\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.hist(stu_stats['Pct Hint'], bins=10)\n",
    "plt.title(\"Pct Hint Tx\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.hist(tx['duration'],bins=50)\n",
    "plt.title(\"Tx duration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 10\n",
    "num_rows = 3\n",
    "num_cols = 6\n",
    "row_height = 3\n",
    "col_width = 3\n",
    "plt.figure(figsize=(col_width*num_cols, row_height*num_rows+num_rows))\n",
    "\n",
    "row = 0\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 1)\n",
    "plt.hist(stu_prob_stats['Step Attempt mean'], bins=num_bins)\n",
    "plt.title(\"Mean Attempts per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 2)\n",
    "plt.hist(stu_prob_stats['Step Attempt std'], bins=num_bins)\n",
    "plt.title(\"Standard Dev Attempts per step\")\n",
    "\n",
    "\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 3)\n",
    "plt.hist(stu_prob_stats['Step Attempt 25%'], bins=num_bins)\n",
    "plt.title(\"Q1 Attempts per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 4)\n",
    "plt.hist(stu_prob_stats['Step Attempt 50%'], bins=num_bins)\n",
    "plt.title(\"Q2 Attempts per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 5)\n",
    "plt.hist(stu_prob_stats['Step Attempt 75%'], bins=num_bins)\n",
    "plt.title(\"Q3 Attempts per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 6)\n",
    "plt.hist(stu_prob_stats['Step Attempt max'], bins=num_bins)\n",
    "plt.title(\"Max Attempts per step\")\n",
    "\n",
    "row = 1\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 1)\n",
    "plt.hist(stu_prob_stats['Step Duration mean'], bins=num_bins)\n",
    "plt.title(\"Mean Time per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 2)\n",
    "plt.hist(stu_prob_stats['Step Duration std'], bins=num_bins)\n",
    "plt.title(\"Standard Dev Time per step\")\n",
    "\n",
    "\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 3)\n",
    "plt.hist(stu_prob_stats['Step Duration 25%'], bins=num_bins)\n",
    "plt.title(\"Q1 Time per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 4)\n",
    "plt.hist(stu_prob_stats['Step Duration 50%'], bins=num_bins)\n",
    "plt.title(\"Q2 Time per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 5)\n",
    "plt.hist(stu_prob_stats['Step Duration 75%'], bins=num_bins)\n",
    "plt.title(\"Q3 Time per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 6)\n",
    "plt.hist(stu_prob_stats['Step Duration max'], bins=num_bins)\n",
    "plt.title(\"Max Time per step\")\n",
    "\n",
    "row = 2\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 1)\n",
    "plt.hist(kc_stats[('kc opportunities', 'mean')], bins=num_bins)\n",
    "plt.title(\"Mean opportunities per kc\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 2)\n",
    "plt.hist(kc_stats[('kc opportunities', 'std')], bins=num_bins)\n",
    "plt.title(\"Standard Dev opportunities per kc\")\n",
    "\n",
    "\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 3)\n",
    "plt.hist(kc_stats[('kc opportunities', '25%')], bins=num_bins)\n",
    "plt.title(\"Q1 opportunities per kc\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 4)\n",
    "plt.hist(kc_stats[('kc opportunities', '50%')], bins=num_bins)\n",
    "plt.title(\"Q2 opportunities per kc\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 5)\n",
    "plt.hist(kc_stats[('kc opportunities', '75%')], bins=num_bins)\n",
    "plt.title(\"Q3 opportunities per kc\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 6)\n",
    "plt.hist(kc_stats[('kc opportunities', 'max')], bins=num_bins)\n",
    "plt.title(\"Max opportunities per kc\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diligent with domain-level self-eff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = sids[batch_desc[2]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [db, calc.get_stu_parameters, sid, 10]\n",
    "sim_students, runtime = time_calc(batch_calc, args)\n",
    "logger.info(f\"Calculated student params: {sim_students.shape}\\tRuntime: {runtime} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [db, calc.action_stats, sid, 10]\n",
    "action_dist, runtime = time_calc(batch_calc, args)\n",
    "logger.info(f\"Calculated student action stats: {action_dist.shape}\\tRuntime: {runtime} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [db, calc.total_tx_stats, sid, 10]\n",
    "tx_stats, runtime = time_calc(batch_calc, args)\n",
    "logger.info(f\"Calculated student activity stats: {tx_stats.shape}\\tRuntime: {runtime} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "stu_stats = pd.concat([sim_students, action_dist, tx_stats], axis=1)\n",
    "logger.info(f\"Merged new stats together: {stu_stats.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_students.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mastery\n",
    "plt.figure(figsize=(10,2))\n",
    "plt.subplot(131)\n",
    "plt.hist(sim_students['pre-sim pct mastery'], bins=10)\n",
    "plt.title(\"pre-sim pct mastery\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hist(sim_students['final-sim pct mastery'], bins=10)\n",
    "plt.title(\"final-sim pct mastery\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.hist(sim_students['final-sim total unmastered'], bins=10)\n",
    "plt.title(\"final-sim total unmastered\")\n",
    "\n",
    "plt.show()\n",
    "logger.info(\"Total skills: %i\" % sim_students['total skills'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Values & Diligence\n",
    "plt.figure(figsize=(8,7))\n",
    "for i,val in enumerate(sim_students['values'][0].keys()):\n",
    "    print(\"%i: %s\" % (i, val))\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.hist(sim_students['values'].apply(lambda x: x[val]), bins=10)\n",
    "    plt.title('Student value for %s' % val)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.hist(sim_students['diligence'], bins=10)\n",
    "plt.title(\"Diligence\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learner Actions EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,2))\n",
    "plt.subplot(141)\n",
    "plt.hist(action_dist['Pct Attempt'], bins=10)\n",
    "plt.title(\"Pct Attempt\")\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.hist(action_dist['Pct Guess'], bins=10)\n",
    "plt.title(\"Pct Guess\")\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.hist(action_dist['Pct Hint Request'], bins=10)\n",
    "plt.title(\"Pct Hint Request\")\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.hist(action_dist['Pct Off Task'], bins=10)\n",
    "plt.title(\"Pct Off Task\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learner Transactions EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [db, get_tx_fields, sid, 10]\n",
    "tx, runtime = time_calc(batch_calc, args)\n",
    "logger.info(f\"Extracted tx for set of students in {runtime} seconds: {tx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_stats = tx.groupby(['stu_id', 'unit_id', 'section_id', 'prob_id', 'step_id'])['duration'].agg(['sum', 'count']).reset_index()\n",
    "stu_prob_stats = step_stats.groupby('stu_id')['count'].describe()\n",
    "stu_prob_stats.columns = [\"Step Attempt %s\" % col for col in stu_prob_stats.columns]\n",
    "d = step_stats.groupby('stu_id')['sum'].describe()\n",
    "d.columns = [\"Step Duration %s\" % col for col in d.columns]\n",
    "stu_prob_stats = pd.concat([stu_prob_stats, d], axis=1)\n",
    "stu_prob_stats.head()\n",
    "\n",
    "# kc_stats = tx[['stu_id', 'kc', 'step_id']].drop_duplicates().groupby(['stu_id', 'kc']).count()\n",
    "stu_kc_stats = tx[['stu_id', 'kc', 'step_id']].drop_duplicates().groupby(['stu_id', 'kc']).count().reset_index()\n",
    "stu_kc_stats.rename(columns={'step_id': 'kc opportunities'}, inplace=True)\n",
    "kc_stats = stu_kc_stats.groupby('kc').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,2))\n",
    "plt.subplot(121)\n",
    "plt.hist(stu_stats['Total Tx'], bins=10)\n",
    "plt.title(\"Total Tx\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(stu_stats['Total Time(hours)'], bins=10)\n",
    "plt.title(\"Total Time(hours)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,2))\n",
    "plt.subplot(131)\n",
    "plt.hist(stu_stats['Pct Correct'], bins=10)\n",
    "plt.title(\"Pct Correct Tx\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hist(stu_stats['Pct Incorrect'], bins=10)\n",
    "plt.title(\"Pct Incorrect Tx\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.hist(stu_stats['Pct Hint'], bins=10)\n",
    "plt.title(\"Pct Hint Tx\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.hist(tx['duration'],bins=50)\n",
    "plt.title(\"Tx duration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 10\n",
    "num_rows = 3\n",
    "num_cols = 6\n",
    "row_height = 3\n",
    "col_width = 3\n",
    "plt.figure(figsize=(col_width*num_cols, row_height*num_rows+num_rows))\n",
    "\n",
    "row = 0\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 1)\n",
    "plt.hist(stu_prob_stats['Step Attempt mean'], bins=num_bins)\n",
    "plt.title(\"Mean Attempts per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 2)\n",
    "plt.hist(stu_prob_stats['Step Attempt std'], bins=num_bins)\n",
    "plt.title(\"Standard Dev Attempts per step\")\n",
    "\n",
    "\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 3)\n",
    "plt.hist(stu_prob_stats['Step Attempt 25%'], bins=num_bins)\n",
    "plt.title(\"Q1 Attempts per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 4)\n",
    "plt.hist(stu_prob_stats['Step Attempt 50%'], bins=num_bins)\n",
    "plt.title(\"Q2 Attempts per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 5)\n",
    "plt.hist(stu_prob_stats['Step Attempt 75%'], bins=num_bins)\n",
    "plt.title(\"Q3 Attempts per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 6)\n",
    "plt.hist(stu_prob_stats['Step Attempt max'], bins=num_bins)\n",
    "plt.title(\"Max Attempts per step\")\n",
    "\n",
    "row = 1\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 1)\n",
    "plt.hist(stu_prob_stats['Step Duration mean'], bins=num_bins)\n",
    "plt.title(\"Mean Time per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 2)\n",
    "plt.hist(stu_prob_stats['Step Duration std'], bins=num_bins)\n",
    "plt.title(\"Standard Dev Time per step\")\n",
    "\n",
    "\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 3)\n",
    "plt.hist(stu_prob_stats['Step Duration 25%'], bins=num_bins)\n",
    "plt.title(\"Q1 Time per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 4)\n",
    "plt.hist(stu_prob_stats['Step Duration 50%'], bins=num_bins)\n",
    "plt.title(\"Q2 Time per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 5)\n",
    "plt.hist(stu_prob_stats['Step Duration 75%'], bins=num_bins)\n",
    "plt.title(\"Q3 Time per step\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 6)\n",
    "plt.hist(stu_prob_stats['Step Duration max'], bins=num_bins)\n",
    "plt.title(\"Max Time per step\")\n",
    "\n",
    "row = 2\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 1)\n",
    "plt.hist(kc_stats[('kc opportunities', 'mean')], bins=num_bins)\n",
    "plt.title(\"Mean opportunities per kc\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 2)\n",
    "plt.hist(kc_stats[('kc opportunities', 'std')], bins=num_bins)\n",
    "plt.title(\"Standard Dev opportunities per kc\")\n",
    "\n",
    "\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 3)\n",
    "plt.hist(kc_stats[('kc opportunities', '25%')], bins=num_bins)\n",
    "plt.title(\"Q1 opportunities per kc\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 4)\n",
    "plt.hist(kc_stats[('kc opportunities', '50%')], bins=num_bins)\n",
    "plt.title(\"Q2 opportunities per kc\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 5)\n",
    "plt.hist(kc_stats[('kc opportunities', '75%')], bins=num_bins)\n",
    "plt.title(\"Q3 opportunities per kc\")\n",
    "plt.subplot(num_rows,num_cols,num_cols*row + 6)\n",
    "plt.hist(kc_stats[('kc opportunities', 'max')], bins=num_bins)\n",
    "plt.title(\"Max opportunities per kc\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Estimating Diligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get students batches\n",
    "batches = [batch for batch in db.simbatches.find()]\n",
    "batch_desc = [\"Simple diligent students\",\n",
    "              \"Diligent Students with variable values\",\n",
    "              \"Diligent Students with domain-level self-efficacy\"]\n",
    "sids = {desc: [] for desc in batch_desc}\n",
    "for i, batch in enumerate(batches):\n",
    "    logger.info(f\"batch #{i}: \\nID: {batch['_id']}\\ndesc: {batch['desc']}\")\n",
    "    if batch['desc'] in batch_desc:\n",
    "        logger.info(f\"recovered {len(batch['student_ids'])} student ids for batch {batch['desc']}\")\n",
    "        sids[batch['desc']].append(batch['student_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random train-test split\n",
    "test_ratio = 0.2\n",
    "tt_split_sids = {}\n",
    "for desc, sid in sids.items():\n",
    "    n = len(sid[0])\n",
    "    ntest = round(test_ratio*n)\n",
    "    ntrain = n - ntest\n",
    "    logger.info(f\"Spliting data batch {desc}\\nTotal: {n}\\tTest: {ntest}\\tTrain: {ntrain}\")\n",
    "    \n",
    "                \n",
    "    test_sids = random.sample(sid[0], ntest)\n",
    "    train_sids = [s for s in sid[0] if s not in test_sids]\n",
    "    tt_split_sids[desc] = {'train': train_sids, 'test': test_sids}\n",
    "\n",
    "def get_train_data(desc):\n",
    "    return tt_split_sids[desc]['train']\n",
    "\n",
    "def get_test_data(desc):\n",
    "    return tt_split_sids[desc]['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch analytic calculation\n",
    "def batch_calc(db, calc, filt, batch_size, ):\n",
    "    #filt is a list of ids\n",
    "    batches = (filt[i:i+batch_size] for i in range(0, len(filt), batch_size))\n",
    "    results = []\n",
    "    for batch in batches:\n",
    "        results.append(calc(batch))\n",
    "    return pd.concat(results, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_calc(calc, args):\n",
    "    start = dt.datetime.now()\n",
    "    result = calc(*args)\n",
    "    end = dt.datetime.now()\n",
    "    runtime = (end - start).total_seconds()\n",
    "    return result, runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating stats with analytic methods\n",
    "\n",
    "calc = StudentStatCalc(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stu_stats = {desc: {} for desc in tt_split_sids.keys()}\n",
    "for desc, split_d in tt_split_sids.items():\n",
    "    for key, d in split_d.items():\n",
    "        logger.info(f\"Calculating for student set {desc}\\t {key}-split\")\n",
    "        args = [db, calc.get_stu_parameters, d, 10]\n",
    "        sim_students, runtime = time_calc(batch_calc, args)\n",
    "        logger.info(f\"Calculated student params: {sim_students.shape}\\tRuntime: {runtime} seconds\")\n",
    "\n",
    "        args = [db, calc.action_stats, d, 10]\n",
    "        action_dist, runtime = time_calc(batch_calc, args)\n",
    "        logger.info(f\"Calculated student action stats: {action_dist.shape}\\tRuntime: {runtime} seconds\")\n",
    "\n",
    "        args = [db, calc.total_tx_stats, d, 10]\n",
    "        tx_stats, runtime = time_calc(batch_calc, args)\n",
    "        logger.info(f\"Calculated student activity stats: {tx_stats.shape}\\tRuntime: {runtime} seconds\")\n",
    "\n",
    "\n",
    "        stu_stats[desc][key] = pd.concat([sim_students, action_dist, tx_stats], axis=1)\n",
    "        logger.info(f\"Merged new stats together: {stu_stats[desc][key].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time on Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcol = \"diligence\"\n",
    "ycol = \"Total Time(hours)\"\n",
    "\n",
    "for desc, splits in stu_stats.items():\n",
    "    i=0\n",
    "    plt.figure(figsize=(12,3))\n",
    "    for split, stat in splits.items():\n",
    "        plt.subplot(1,len(splits.keys()),i+1)\n",
    "        plt.title(f\"{desc}-{split} data\")\n",
    "        plt.scatter(stat[xcol], stat[ycol])\n",
    "        plt.xlabel(xcol)\n",
    "        plt.ylabel(ycol)\n",
    "        i += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcol = \"pre-sim pct mastery\"\n",
    "ycol = \"Total Time(hours)\"\n",
    "\n",
    "for desc, splits in stu_stats.items():\n",
    "    i=0\n",
    "    plt.figure(figsize=(12,3))\n",
    "    for split, stat in splits.items():\n",
    "        plt.subplot(1,len(splits.keys()),i+1)\n",
    "        plt.title(f\"{desc}-{split} data\")\n",
    "        plt.scatter(stat[xcol], stat[ycol])\n",
    "        plt.xlabel(xcol)\n",
    "        plt.ylabel(ycol)\n",
    "        i += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freq of Off-task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcol = \"Off Task\"\n",
    "ycol = \"diligence\"\n",
    "\n",
    "for desc, splits in stu_stats.items():\n",
    "    i=0\n",
    "    plt.figure(figsize=(12,3))\n",
    "    for split, stat in splits.items():\n",
    "        d1 = stat[xcol]\n",
    "        d2 = stat[ycol]\n",
    "        corr, pval = pearsonr(d1, d2)\n",
    "        logger.info(f\"student set '{desc}'-{split}: R = {corr}\\t pval = {pval}\")\n",
    "        plt.subplot(1,len(splits.keys()),i+1)\n",
    "        plt.title(f\"{desc}-{split} data\")\n",
    "        plt.scatter(stat[xcol], stat[ycol])\n",
    "        plt.xlabel(xcol)\n",
    "        plt.ylabel(ycol)\n",
    "        i += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for desc, splits in stu_stats.items():\n",
    "    stat = splits['train']\n",
    "    X_train = stat.loc[:, [\"Off Task\"]]\n",
    "    X_train = sm.add_constant(X_train)\n",
    "    Y_train = stat.loc[:, \"diligence\"]\n",
    "    model = sm.OLS(Y_train, X_train)\n",
    "    result = model.fit()\n",
    "    Y_hat_train = result.predict(X_train)\n",
    "    err_train = mean_squared_error(Y_train, Y_hat_train)\n",
    "    logger.info(\"****************************************************************\")\n",
    "    logger.info(f\"Model fit for {desc} -- Training data\")\n",
    "    logger.info(result.summary())\n",
    "    #logger.info(result.t_test([1]))\n",
    "    \n",
    "#    sm.graphics.plot_partregress(\"diligence\", \"Off Task\", [\"Off Task\"], data=stat)\n",
    "    \n",
    "    stat = splits['test']\n",
    "    X_test = stat.loc[:, [\"Off Task\"]]\n",
    "    X_test = sm.add_constant(X_test)\n",
    "\n",
    "    Y_test = stat.loc[:, \"diligence\"]\n",
    "    Y_hat_test = result.predict(X_test)\n",
    "    \n",
    "    err_test = mean_squared_error(Y_test, Y_hat_test)\n",
    "    logger.info(f\"--- MSE train: {err_train}\\t test: {err_test} ---\")\n",
    "    \n",
    "    xmin = np.min([np.min([np.min(Y_train), np.min(Y_test)]), np.min([np.min(Y_hat_train), np.min(Y_hat_test)])])\n",
    "    xmax = np.max([np.max([np.max(Y_train), np.max(Y_test)]), np.max([np.max(Y_hat_train), np.max(Y_hat_test)])])\n",
    "\n",
    "#    xmax = np.max([np.max(Y_train), np.max(Y_test), np.max(Y_hat_train), np.max(Y_hat_test)])\n",
    "    ymin = xmin\n",
    "    ymax = xmax\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10,9))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.scatter(Y_train, Y_hat_train)\n",
    "    plt.title(f\"{desc} -- train est\")\n",
    "    plt.xlabel(\"Diligence\")\n",
    "    plt.ylabel(\"Estimated Diligence\")\n",
    "    plt.xlim([xmin, xmax])\n",
    "    plt.ylim([ymin, ymax])\n",
    "    \n",
    "    plt.subplot(2,2,2)\n",
    "    plt.scatter(Y_test, Y_hat_test)\n",
    "    plt.title(f\"{desc} -- test est\")\n",
    "    plt.xlabel(\"Diligence\")\n",
    "    plt.ylabel(\"Estimated Diligence\")\n",
    "    plt.xlim([xmin, xmax])\n",
    "    plt.ylim([ymin, ymax])\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.title(f\"{desc} -- train err\")\n",
    "    plt.scatter(Y_train, Y_train - Y_hat_train)\n",
    "    plt.xlabel(\"Diligence\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    \n",
    "    plt.subplot(2,2,4)\n",
    "    plt.title(f\"{desc} -- test err\")\n",
    "              \n",
    "    plt.scatter(Y_test, Y_test - Y_hat_test)\n",
    "    plt.xlabel(\"Diligence\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportion of Off-task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcol = \"Pct Off Task\"\n",
    "ycol = \"diligence\"\n",
    "\n",
    "\n",
    "for desc, splits in stu_stats.items():\n",
    "    i=0\n",
    "    plt.figure(figsize=(12,3))\n",
    "    for split, stat in splits.items():\n",
    "        d1 = stat[xcol]\n",
    "        d2 = stat[ycol]\n",
    "        corr, pval = pearsonr(d1, d2)\n",
    "        logger.info(f\"student set '{desc}'-{split}: R = {corr}\\t pval = {pval}\")\n",
    "        plt.subplot(1,len(splits.keys()),i+1)\n",
    "        plt.title(f\"{desc}-{split} data\")\n",
    "        plt.scatter(stat[xcol], stat[ycol])\n",
    "        plt.xlabel(xcol)\n",
    "        plt.ylabel(ycol)\n",
    "        i += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcols = [\"Pct Off Task\"]\n",
    "ycols = \"diligence\"\n",
    "\n",
    "for desc, splits in stu_stats.items():\n",
    "    stat = splits['train']\n",
    "    \n",
    "    # Extract student intrinsic EV value of Off Task\n",
    "    stat[val_col] = stat.apply(lambda x: x['values'][\"off task\"], axis=1)\n",
    "    \n",
    "    X_train = stat.loc[:, xcols]\n",
    "    X_train = sm.add_constant(X_train)\n",
    "    Y_train = stat.loc[:, ycol]\n",
    "    model = sm.OLS(Y_train, X_train)\n",
    "    result = model.fit()\n",
    "    Y_hat_train = result.predict(X_train)\n",
    "    err_train = mean_squared_error(Y_train, Y_hat_train)\n",
    "    logger.info(\"****************************************************************\")\n",
    "    logger.info(f\"Model fit for {desc} -- Training data\")\n",
    "    logger.info(result.summary())\n",
    "    #logger.info(result.t_test([1]))\n",
    "    \n",
    "#    sm.graphics.plot_partregress(\"diligence\", \"Off Task\", [\"Off Task\"], data=stat)\n",
    "    \n",
    "    stat = splits['test']\n",
    "    \n",
    "    # Extract student intrinsic EV value of Off Task\n",
    "    stat[val_col] = stat.apply(lambda x: x['values'][\"off task\"], axis=1)\n",
    "    \n",
    "    X_test = stat.loc[:, xcols]\n",
    "    X_test = sm.add_constant(X_test)\n",
    "\n",
    "    Y_test = stat.loc[:, ycol]\n",
    "    Y_hat_test = result.predict(X_test)\n",
    "    \n",
    "    err_test = mean_squared_error(Y_test, Y_hat_test)\n",
    "    logger.info(f\"--- MSE train: {err_train}\\t test: {err_test} ---\")\n",
    "    \n",
    "    xmin = np.min([np.min([np.min(Y_train), np.min(Y_test)]), np.min([np.min(Y_hat_train), np.min(Y_hat_test)])])\n",
    "    xmax = np.max([np.max([np.max(Y_train), np.max(Y_test)]), np.max([np.max(Y_hat_train), np.max(Y_hat_test)])])\n",
    "\n",
    "#    xmax = np.max([np.max(Y_train), np.max(Y_test), np.max(Y_hat_train), np.max(Y_hat_test)])\n",
    "    ymin = xmin\n",
    "    ymax = xmax\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10,9))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.scatter(Y_train, Y_hat_train)\n",
    "    plt.title(f\"{desc} -- train est\")\n",
    "    plt.xlabel(\"Diligence\")\n",
    "    plt.ylabel(\"Estimated Diligence\")\n",
    "    plt.xlim([xmin, xmax])\n",
    "    plt.ylim([ymin, ymax])\n",
    "    \n",
    "    plt.subplot(2,2,2)\n",
    "    plt.scatter(Y_test, Y_hat_test)\n",
    "    plt.title(f\"{desc} -- test est\")\n",
    "    plt.xlabel(\"Diligence\")\n",
    "    plt.ylabel(\"Estimated Diligence\")\n",
    "    plt.xlim([xmin, xmax])\n",
    "    plt.ylim([ymin, ymax])\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.title(f\"{desc} -- train err\")\n",
    "    plt.scatter(Y_train, Y_train - Y_hat_train)\n",
    "    plt.xlabel(\"Diligence\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    \n",
    "    plt.subplot(2,2,4)\n",
    "    plt.title(f\"{desc} -- test err\")\n",
    "              \n",
    "    plt.scatter(Y_test, Y_test - Y_hat_test)\n",
    "    plt.xlabel(\"Diligence\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Pct Off Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_col = \"Value: Off Task\"\n",
    "xcols = [\"diligence\", val_col]\n",
    "ycol = \"Pct Off Task\"\n",
    "\n",
    "desc = list(stu_stats.keys())[1]\n",
    "stat = stu_stats[desc]['train'].copy()\n",
    "\n",
    "# Extract student intrinsic EV value of Off Task\n",
    "stat[val_col] = stat.apply(lambda x: x['values'][\"off task\"], axis=1)\n",
    "\n",
    "X_train = stat.loc[:, xcols]\n",
    "X_means = X_train.mean()\n",
    "X_std = X_train.std()\n",
    "X_train = (X_train - X_means) / X_std\n",
    "X_train = sm.add_constant(X_train)\n",
    "Y_train = stat.loc[:, ycol]\n",
    "Y_mean = Y_train.mean()\n",
    "Y_std = Y_train.std()\n",
    "Y_Train = (Y_train - Y_mean) / Y_std\n",
    "model = sm.OLS(Y_train, X_train)\n",
    "result = model.fit()\n",
    "Y_hat_train = result.predict(X_train)\n",
    "err_train = mean_squared_error(Y_train, Y_hat_train)\n",
    "logger.info(\"****************************************************************\")\n",
    "logger.info(f\"Model fit for {desc} -- Training data\")\n",
    "logger.info(result.summary())\n",
    "#logger.info(result.t_test([1]))\n",
    "\n",
    "stat = stu_stats[desc]['test'].copy()\n",
    "\n",
    "# Extract student intrinsic EV value of Off Task\n",
    "stat[val_col] = stat.apply(lambda x: x['values'][\"off task\"], axis=1)\n",
    "\n",
    "X_test = stat.loc[:, xcols]\n",
    "X_test = (X_test - X_means) / X_std\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "Y_test = stat.loc[:, ycol]\n",
    "Y_test = (Y_test - Y_mean) / Y_std\n",
    "Y_hat_test = result.predict(X_test)\n",
    "\n",
    "err_test = mean_squared_error(Y_test, Y_hat_test)\n",
    "logger.info(f\"--- MSE train: {err_train}\\t test: {err_test} ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student-level CAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_student_cae(d, penalty1, penalty):\n",
    "    logger.debug(f\"calculating CAE on data with shape: {d.shape}\")\n",
    "    stu_id = d['stu_id'][0]\n",
    "    data_proc = SimpleCAEPreprocessor(d)\n",
    "    d_proc = data_proc.process_data()\n",
    "\n",
    "    col_names = d_proc.columns.tolist()\n",
    "    logger.debug(f\"computing cae on dataframe: {d_proc.shape}\")\n",
    "    data_idx = d['_id']\n",
    "    caa = StudentCAAModel.from_caa_obj(CAAComputation(d_proc.to_numpy(), penalty1, penalty2), data_proc, data_idx, stu_id)\n",
    "    return caa, col_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_student_caes(sids, db, batch_desc, penalty1, penalty2):\n",
    "    # Calculate CAA embedding for each student\n",
    "    logger.debug(\"Building CAA embedding for each student\")\n",
    "    \n",
    "    # Get data batches to process from db\n",
    "    col = \"tutor_events\"\n",
    "    base_query = {\"stu_id\": {\"$in\": sids},\n",
    "                  \"type\": \"TutorInput\"\n",
    "                 }\n",
    "    logger.info(f\"{col} collection has {db[col].count_documents(base_query)} documents associated with {len(sids)} students\")\n",
    "    logger.debug(f\"Using query: {base_query}\")\n",
    "    segmenter = Segmenter(db[col], base_query)\n",
    "    idx_fields = ['stu_id']\n",
    "    caa_batches = segmenter.get_batches(idx_fields, 1)\n",
    "    caa_mdls = []\n",
    "    col_names = []\n",
    "    caa_batch = CAABatch(batch_desc, col_names)\n",
    "\n",
    "    # Calculate CAA embeddings\n",
    "    i = 0\n",
    "    for query, batch in caa_batches:\n",
    "        if i%25 == 0:\n",
    "            logger.info(f\"Calculating CAE on Student #{i}\")\n",
    "        caa, col_names = calc_student_cae(batch, penalty1, penalty2)\n",
    "        caa_batch.add(caa)\n",
    "        i += 1\n",
    "\n",
    "    caa_batch.col_names = col_names\n",
    "    \n",
    "    return caa_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_caes(caa_batch, eps, min_samples):\n",
    "    logger.info(f\"****** Clustering with DBSCAN with EPS={eps} and min_sample={min_samples}******\")\n",
    "    X = caa_batch.get_distances().to_numpy()\n",
    "\n",
    "\n",
    "    clusterer = DBSCAN(eps=eps, min_samples=min_samples, metric=\"precomputed\")\n",
    "    clusters = clusterer.fit(X)\n",
    "    labels = clusters.labels_\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "    logger.info('Estimated number of clusters: %d' % n_clusters_)\n",
    "    logger.info('Estimated number of noise points: %d' % n_noise_)\n",
    "    logger.info(f\"Cluster label counts: {pd.Series(clusters.labels_).value_counts()}\")\n",
    "    \n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch_DBSCAN_hyperparams(d, eps_range, min_samples_range):\n",
    "    for eps in eps_range:\n",
    "        for min_samples in min_samples_range:\n",
    "            clusters = cluster_caes(d, eps, min_samples)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cae_clusters(caa_batch, eps, min_samples):\n",
    "    # Map cluster labels with projection ids and original student ids\n",
    "    clusters = cluster_caes(caa_batch, eps, min_samples)\n",
    "    y = pd.DataFrame({\"proj_id\": caa_batch.get_index(), \"cluster\": clusters.labels_})\n",
    "    pid_map = {pid: proj.caa_model_id for pid, proj in caa_batch.projections.items()}\n",
    "    caa_map = {mdl._id: mdl for mdl in caa_batch.mdls}\n",
    "    pids = pid_map.keys()\n",
    "    caa_mids = [pid_map[pid] for pid in pids]\n",
    "    sids = [caa_map[mid].student_id for mid in caa_mids]\n",
    "    pid_map = pd.DataFrame({\"proj_id\": pids, \"caa_model_id\": caa_mids, \"student_id\": sids})\n",
    "\n",
    "    # Assemble dataframe with CAE projections for each student\n",
    "    logger.debug(f\"premerge shape: {y.shape}\")\n",
    "    y = pd.merge(y, pid_map, on=\"proj_id\", how=\"inner\")\n",
    "    logger.debug(f\"post-merge shape: {y.shape}\")\n",
    "    cluster_counts = y['cluster'].value_counts()\n",
    "    for lbl in cluster_counts.index: \n",
    "        logger.info(f\"Cluster label: {lbl}\\t count: {cluster_counts[lbl]}\")\n",
    "     \n",
    "    # Add columns for each projection axis(u&v) and weighting factor(d)\n",
    "    y.loc[:, 'US'] = y.apply(lambda x: caa_batch.projections[x['proj_id']].u, axis=1)\n",
    "    y.loc[:, 'VS'] = y.apply(lambda x: caa_batch.projections[x['proj_id']].v, axis=1)\n",
    "    y.loc[:, 'd'] = y.apply(lambda x: caa_batch.projections[x['proj_id']].d[0], axis=1)\n",
    "\n",
    "    # Expand CAE projection axis components into individual columns\n",
    "    us = y.apply(lambda x: pd.Series(x['US'][0].tolist(), index=caa_batch.col_names), axis=1)\n",
    "    new_colnames = {col: f\"u-{col}\" for col in us.columns}\n",
    "    u_cols = new_colnames.values()\n",
    "    us.rename(columns=new_colnames, inplace=True)\n",
    "    vs = y.apply(lambda x: pd.Series(x['VS'][0].tolist(), index=caa_batch.col_names), axis=1)\n",
    "    new_colnames = {col: f\"v-{col}\" for col in vs.columns}\n",
    "    v_cols = new_colnames.values()\n",
    "    vs.rename(columns=new_colnames, inplace=True)\n",
    "    y1 = pd.concat([y, us, vs], axis=1)\n",
    "    logger.info(y1.columns)\n",
    "    \n",
    "    return y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cae_means(cae, col_names):\n",
    "    #Calculate the mean of each cluster\n",
    "    #us = cae.apply(lambda x: pd.Series(x['US'][0].tolist(), index=col_names), axis=1)\n",
    "    new_colnames = {col: f\"u-{col}\" for col in col_names}\n",
    "    u_cols = new_colnames.values()\n",
    "    #us.rename(columns=new_colnames, inplace=True)\n",
    "    #vs = cae.apply(lambda x: pd.Series(x['VS'][0].tolist(), index=col_names), axis=1)\n",
    "    new_colnames = {col: f\"v-{col}\" for col in col_names}\n",
    "    v_cols = new_colnames.values()\n",
    "    #vs.rename(columns=new_colnames, inplace=True)\n",
    "    \n",
    "    cluster_center = cae.groupby('cluster')[list(u_cols) + list(v_cols)].mean()\n",
    "    return cluster_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_df(cae, stu_dict, col_names=None):\n",
    "    stu_cae = cae.pivot_table(values='d', index='student_id', columns='cluster', fill_value=0, aggfunc=np.sum)\n",
    "    stu_cae.rename(columns={col: f\"cluster_{col}\" for col in stu_cae.columns}, inplace=True)\n",
    "    \n",
    "    if col_names is not None:\n",
    "        # Ensure all given columns are present\n",
    "        for col in col_names:\n",
    "            if 'cluster' in col and col not in stu_cae.columns:\n",
    "                stu_cae[col] = np.zeros(stu_cae.shape[0])\n",
    "        \n",
    "    \n",
    "    stu_cae['diligence'] = pd.Series([stu_dict[sid].decider.diligence for sid in stu_cae.index.tolist()], index=stu_cae.index)\n",
    "\n",
    "    # Drop the noise cluster (-1)\n",
    "    drop_col = 'cluster_-1'\n",
    "    if drop_col in stu_cae.columns:\n",
    "        stu_cae.drop(columns=[drop_col], inplace=True) \n",
    "    \n",
    "    return stu_cae\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caa_batches = {desc: {\"train\": None, 'test': None} for desc in batch_desc}\n",
    "\n",
    "for bdesc in batch_desc:\n",
    "    train_sids = get_train_data(bdesc)\n",
    "    test_sids = get_test_data(bdesc) \n",
    "    \n",
    "    lmapper = DBLearnerMapper(db)\n",
    "    students = [lmapper.get_modlearner_from_db(stu_id) for stu_id in train_sids]\n",
    "    stu_dict = {stu._id: stu for stu in students}\n",
    "    \n",
    "    eps = 0.8\n",
    "    min_samples = 5\n",
    "    penalty1 = 0.35\n",
    "    penalty2 = 0.35\n",
    "    \n",
    "    caa_batches[bdesc]['train'] = get_student_caes(train_sids, db, bdesc, penalty1, penalty2)\n",
    "\n",
    "    # Presist CAE Models\n",
    "    db.caa_models.insert_many([mdl.to_dict() for mdl in caa_batches[bdesc]['train'].mdls])\n",
    "    db.caa_batches.insert_one(caa_batches[bdesc]['train'].to_dict())\n",
    "\n",
    "    caa_batches[bdesc]['test'] = get_student_caes(test_sids, db, bdesc, penalty1, penalty2)\n",
    "\n",
    "    # Presist CAE Models\n",
    "    db.caa_models.insert_many([mdl.to_dict() for mdl in caa_batches[bdesc]['test'].mdls])\n",
    "    db.caa_batches.insert_one(caa_batches[bdesc]['test'].to_dict())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster/Label Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caa_clusters = {desc: {'train_clusters': None, 'train_distances': None, 'test_distances': None, 'test_clusters': None} for desc in batch_desc}\n",
    "for bdesc in batch_desc:\n",
    "    # Cluster training data    \n",
    "    test_batch = caa_batches[bdesc]['test']\n",
    "    train_batch = caa_batches[bdesc]['train']\n",
    "    \n",
    "    stu_cae = get_cae_clusters(train_batch, eps, min_samples)\n",
    "    \n",
    "    cluster_centers = calc_cae_means(stu_cae, train_batch.col_names)\n",
    "    logger.info(f\"***************** Cluster centers for set: {bdesc} *****************\")\n",
    "    logger.info(cluster_centers.head(cluster_centers.shape[0]))\n",
    "    \n",
    "    train_distances = train_batch.get_distances()\n",
    "    \n",
    "    X = train_distances\n",
    "    train_cluster_labels = stu_cae.loc[:, ['proj_id', 'cluster']].copy()\n",
    "    train_cluster_labels.index = train_cluster_labels['proj_id']\n",
    "    train_cluster_labels.drop(columns=['proj_id'], inplace=True)\n",
    "    Y = pd.Series([train_cluster_labels.loc[pid][0] for pid in X.index], index=X.index)\n",
    "\n",
    "    # Calc distance of test data to train data\n",
    "    rows = test_batch.get_index()\n",
    "    cols = X.index\n",
    "    test_distances = pd.DataFrame(index=rows, columns=cols)\n",
    "    for i in rows:\n",
    "        for j in cols:\n",
    "            p1 = test_batch.projections[i]\n",
    "            p2 = train_batch.projections[j]\n",
    "            d = CAAProjection.distance(p1, p2)\n",
    "            test_distances.loc[i,j] = d\n",
    "    \n",
    "    caa_clusters[bdesc]['train_clusters'] = stu_cae\n",
    "    caa_clusters[bdesc]['train_distances'] = train_distances\n",
    "    caa_clusters[bdesc]['test_distances'] = test_distances\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify Test data\n",
    "n_neighbors = 10\n",
    "\n",
    "for bdesc in batch_desc:\n",
    "    test_batch = caa_batches[bdesc]['test']\n",
    "    train_batch = caa_batches[bdesc]['train']\n",
    "    test_distances = caa_clusters[bdesc]['test_distances']\n",
    "    \n",
    "    X = caa_clusters[bdesc]['train_distances']\n",
    "    train_cluster_labels = caa_clusters[bdesc]['train_clusters'].loc[:, ['proj_id', 'cluster']].copy()\n",
    "    train_cluster_labels.index = train_cluster_labels['proj_id']\n",
    "    train_cluster_labels.drop(columns=['proj_id'], inplace=True)\n",
    "    Y = pd.Series([train_cluster_labels.loc[pid][0] for pid in X.index], index=X.index)\n",
    "    \n",
    "    # Classify test projections with embedding cluster labels\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, metric='precomputed')\n",
    "    clf.fit(X.to_numpy(), Y.to_numpy())\n",
    "    Y_hat_test = clf.predict(test_distances)\n",
    "\n",
    "    #Format test data with labels for feeding to fitted regression model\n",
    "    test_clusters = pd.DataFrame({\"proj_id\": test_distances.index, \"cluster\": Y_hat_test})\n",
    "    pid_map = {pid: proj.caa_model_id for pid, proj in test_batch.projections.items()}\n",
    "    caa_map = {mdl._id: mdl for mdl in test_batch.mdls}\n",
    "    pids = pid_map.keys()\n",
    "    caa_mids = [pid_map[pid] for pid in pids]\n",
    "    sids = [caa_map[mid].student_id for mid in caa_mids]\n",
    "    pid_map = pd.DataFrame({\"proj_id\": pids, \"caa_model_id\": caa_mids, \"student_id\": sids})\n",
    "\n",
    "    # Assemble dataframe with CAE projections for each student\n",
    "    logger.debug(f\"premerge shape: {test_clusters.shape}\")\n",
    "    test_clusters = pd.merge(test_clusters, pid_map, on=\"proj_id\", how=\"inner\")\n",
    "    logger.debug(f\"post-merge shape: {test_clusters.shape}\")\n",
    "    cluster_counts = test_clusters['cluster'].value_counts()\n",
    "    for lbl in cluster_counts.index: \n",
    "        logger.info(f\"Cluster label: {lbl}\\t count: {cluster_counts[lbl]}\")\n",
    "\n",
    "    # Add columns for each projection axis(u&v) and weighting factor(d)\n",
    "    test_clusters.loc[:, 'US'] = test_clusters.apply(lambda x: test_batch.projections[x['proj_id']].u, axis=1)\n",
    "    test_clusters.loc[:, 'VS'] = test_clusters.apply(lambda x: test_batch.projections[x['proj_id']].v, axis=1)\n",
    "    test_clusters.loc[:, 'd'] = test_clusters.apply(lambda x: test_batch.projections[x['proj_id']].d[0], axis=1)\n",
    "\n",
    "    # Expand CAE projection axis components into individual columns\n",
    "    us = test_clusters.apply(lambda x: pd.Series(x['US'][0].tolist(), index=test_batch.col_names), axis=1)\n",
    "    new_colnames = {col: f\"u-{col}\" for col in us.columns}\n",
    "    u_cols = new_colnames.values()\n",
    "    us.rename(columns=new_colnames, inplace=True)\n",
    "    vs = test_clusters.apply(lambda x: pd.Series(x['VS'][0].tolist(), index=test_batch.col_names), axis=1)\n",
    "    new_colnames = {col: f\"v-{col}\" for col in vs.columns}\n",
    "    v_cols = new_colnames.values()\n",
    "    vs.rename(columns=new_colnames, inplace=True)\n",
    "    test_clusters = pd.concat([test_clusters, us, vs], axis=1)\n",
    "    logger.info(test_clusters.columns)\n",
    "    \n",
    "    caa_clusters[bdesc]['test_clusters'] = test_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression estimating Diligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "\n",
    "for bdesc in batch_desc:\n",
    "    stu_cae = caa_clusters[bdesc]['train_clusters']\n",
    "    test_sids = get_test_data(bdesc)\n",
    "    train_sids = get_train_data(bdesc)\n",
    "    test_clusters = caa_clusters[bdesc]['test_clusters']\n",
    "    \n",
    "    train_students = [lmapper.get_modlearner_from_db(stu_id) for stu_id in train_sids]\n",
    "    train_stu_dict = {stu._id: stu for stu in train_students}\n",
    "\n",
    "    # Format training data for regression\n",
    "    train_d = get_reg_df(stu_cae, train_stu_dict)\n",
    "    \n",
    "    # Regression on Training Data\n",
    "    xcols = [col for col in train_d.columns.tolist() if \"cluster\" in col]\n",
    "    ycol = \"diligence\"\n",
    "    X_train = train_d.loc[:, xcols]\n",
    "    Y_train = train_d.loc[:, ycol]\n",
    "    mdl = sm.OLS(Y_train, X_train).fit()\n",
    "    \n",
    "    logger.info(f\"************ Regression for student set: {bdesc} *****************\")\n",
    "    logger.info(mdl.summary())\n",
    "\n",
    "    Y_hat_train = mdl.predict(X_train)\n",
    "    mse_train = mean_squared_error(Y_train, Y_hat_train)\n",
    "\n",
    "    # Predictions of model on Test Data\n",
    "    test_students = [lmapper.get_modlearner_from_db(stu_id) for stu_id in test_sids]\n",
    "    test_stu_dict = {stu._id: stu for stu in test_students}\n",
    "    test_d = get_reg_df(test_clusters, test_stu_dict, train_d.columns)\n",
    "\n",
    "    xcols = [col for col in test_d.columns.tolist() if \"cluster\" in col]\n",
    "    ycol = \"diligence\"\n",
    "    X_test = test_d.loc[:, xcols]\n",
    "    Y_test = test_d.loc[:, ycol]\n",
    "    Y_hat_test = mdl.predict(X_test.to_numpy())\n",
    "    mse_test = mean_squared_error(Y_test, Y_hat_test)\n",
    "\n",
    "\n",
    "    logger.info(f\"Mean square Error. Training: {mse_train}\\t Test: {mse_test}\")\n",
    "    \n",
    "    # Plot Predictions\n",
    "    xmin = np.min([np.min([np.min(Y_train), np.min(Y_test)]), np.min([np.min(Y_hat_train), np.min(Y_hat_test)])])\n",
    "    xmax = np.max([np.max([np.max(Y_train), np.max(Y_test)]), np.max([np.max(Y_hat_train), np.max(Y_hat_test)])])\n",
    "\n",
    "    ymin = xmin\n",
    "    ymax = xmax\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(Y_train, Y_hat_train)\n",
    "    plt.title(f\"{bdesc} -- train est\")\n",
    "    plt.xlabel(\"Diligence\")\n",
    "    plt.ylabel(\"Estimated Diligence\")\n",
    "    plt.xlim([xmin, xmax])\n",
    "    plt.ylim([ymin, ymax])\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.scatter(Y_test, Y_hat_test)\n",
    "    plt.title(f\"{bdesc} -- test est\")\n",
    "    plt.xlabel(\"Diligence\")\n",
    "    plt.ylabel(\"Estimated Diligence\")\n",
    "    plt.xlim([xmin, xmax])\n",
    "    plt.ylim([ymin, ymax])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Diligence Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating Diligence\n",
    "\n",
    "Simple Diligent Students\n",
    "Freq of Off Task\n",
    " R-squared:                       0.268             \n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "const          2.4536      0.066     37.251      0.000       2.323       2.584\n",
    "Off Task      -0.0030      0.000     -7.609      0.000      -0.004      -0.002\n",
    "\n",
    "MSE train: 0.07774522739314112\t test: 0.061369836999304025 ---\n",
    "\n",
    "Proportion of Off Task\n",
    "R-squared:                       0.773\n",
    "================================================================================\n",
    "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
    "--------------------------------------------------------------------------------\n",
    "const            3.5573      0.069     51.542      0.000       3.421       3.694\n",
    "Pct Off Task   -23.7765      1.025    -23.204      0.000     -25.800     -21.753\n",
    "\n",
    "MSE train: 0.024101105255305937\t test: 0.01882725256654274 ---\n",
    "\n",
    "Student-level CAE\n",
    "R-squared:                   0.974                                 \n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "cluster_0      1.9168      0.073     26.214      0.000       1.772       2.061\n",
    "cluster_1      0.0115      0.097      0.118      0.906      -0.181       0.204\n",
    "cluster_2     -0.0849      0.127     -0.671      0.503      -0.335       0.165\n",
    "cluster_3     -0.0509      0.158     -0.322      0.748      -0.364       0.262\n",
    "cluster_4     -0.1387      0.203     -0.683      0.496      -0.540       0.263\n",
    "cluster_5      0.2044      0.358      0.572      0.568      -0.502       0.911\n",
    "cluster_6      0.0392      0.184      0.213      0.832      -0.325       0.403\n",
    "cluster_7     -0.5075      0.416     -1.220      0.225      -1.330       0.315\n",
    "cluster_8      0.3273      0.396      0.827      0.410      -0.455       1.110\n",
    "cluster_9     -0.3256      0.495     -0.658      0.511      -1.303       0.652\n",
    "\n",
    "Mean square Error. Training: 0.10330573607269716\t Test: 0.06715947920995932"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diligent with Self Eff\n",
    "Freq of Off Task\n",
    "R-squared:                       0.117\n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "const          2.2208      0.061     36.417      0.000       2.100       2.341\n",
    "Off Task      -0.0015      0.000     -4.579      0.000      -0.002      -0.001\n",
    "MSE train: 0.07349436743083963\t test: 0.07244362732771051 ---\n",
    "\n",
    "Prportion of Off Task\n",
    "R-squared:                       0.585                                  \n",
    "================================================================================\n",
    "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
    "--------------------------------------------------------------------------------\n",
    "const            3.0817      0.077     40.186      0.000       2.930       3.233\n",
    "Pct Off Task   -17.0734      1.145    -14.912      0.000     -19.335     -14.812\n",
    "MSE train: 0.03457898288803763\t test: 0.03495325673500937 ---\n",
    "\n",
    "Student-level CAE\n",
    "R-squared (uncentered):                   0.978\n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "cluster_0      1.9334      0.110     17.600      0.000       1.716       2.151\n",
    "cluster_1     -0.0632      0.133     -0.477      0.634      -0.326       0.199\n",
    "cluster_2     -0.2080      0.159     -1.305      0.194      -0.523       0.107\n",
    "cluster_3      0.1580      0.132      1.198      0.233      -0.103       0.419\n",
    "cluster_4      0.1984      0.173      1.146      0.254      -0.144       0.541\n",
    "cluster_5      0.3499      0.303      1.156      0.250      -0.249       0.948\n",
    "cluster_6     -0.3941      0.158     -2.493      0.014      -0.707      -0.082\n",
    "cluster_7     -0.5619      0.829     -0.678      0.499      -2.201       1.077\n",
    "cluster_8      0.2437      0.446      0.547      0.585      -0.637       1.125\n",
    "cluster_9     -0.0273      0.364     -0.075      0.940      -0.746       0.692\n",
    "cluster_10     0.1211      0.480      0.253      0.801      -0.827       1.069\n",
    "Mean square Error. Training: 0.08628441695987324\t Test: 0.08708041617099727"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diligent with Variable Values\n",
    "R-squared:                       0.108\n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "const          2.1613      0.036     59.383      0.000       2.089       2.233\n",
    "Off Task      -0.0003   6.19e-05     -4.379      0.000      -0.000      -0.000\n",
    "MSE train: 0.08204510192737742\t test: 0.0999426924595668 ---\n",
    "\n",
    "Proportion of Off Task\n",
    " R-squared:                       0.167    \n",
    "================================================================================\n",
    "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
    "--------------------------------------------------------------------------------\n",
    "const            2.2215      0.039     56.291      0.000       2.144       2.299\n",
    "Pct Off Task    -1.2874      0.229     -5.633      0.000      -1.739      -0.836\n",
    "MSE train: 0.07661662990542648\t test: 0.09838867761509369 ---\n",
    "\n",
    "R-squared (uncentered):                   0.979\n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "cluster_0      1.8089      0.089     20.248      0.000       1.632       1.986\n",
    "cluster_1      0.1390      0.113      1.231      0.220      -0.084       0.362\n",
    "cluster_2     -0.4651      0.159     -2.919      0.004      -0.780      -0.150\n",
    "cluster_3      0.1553      0.160      0.973      0.332      -0.160       0.471\n",
    "cluster_4      0.0198      0.186      0.106      0.915      -0.348       0.387\n",
    "cluster_5     -0.0578      0.251     -0.231      0.818      -0.554       0.438\n",
    "cluster_6     -0.5464      0.369     -1.480      0.141      -1.277       0.184\n",
    "cluster_7      0.4037      0.464      0.869      0.386      -0.514       1.322\n",
    "cluster_8      0.5987      0.342      1.750      0.082      -0.078       1.275\n",
    "cluster_9     -0.2720      0.549     -0.495      0.621      -1.358       0.814\n",
    "Mean square Error. Training: 0.0877327981501877\t Test: 0.1069420444781655"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_out = os.path.join(base_dir, )\n",
    "out_dirs = [\"test\", \"data\", \"simple_compare_dil_est\"]\n",
    "out_path = base_dir\n",
    "for out_dir in out_dirs:\n",
    "    out_path = os.path.join(out_path, out_dir)\n",
    "    if not os.path.exists(out_path):\n",
    "        logger.info(f\"Creating output directory: {out_path}\")\n",
    "        os.mkdir(out_path)\n",
    "logger.info(f\"Writing db output to: {out_path}\")\n",
    "db_util.dump_db(data_dir=out_path)\n",
    "logger.info(\"Done\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
